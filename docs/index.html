
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta charset="utf-8">
	<title>NSF Workshop in SE4Robotics</title>
	<meta name="author" content="drg" >
	<!--[if lt IE 9]>
		<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
	<link href="./css/bootstrap.css" rel="stylesheet">
	<link href="./css/contenido.css" rel="stylesheet">
	<link rel="shortcut icon" href="favicon.ico">
<style type="text/css"></style></head>
<body>

<div class="topbar">
	<div class="topbar-inner">
		<div class="container1">
			<a class="brand" href="http://fse4robotics.github.io/"><abbr
				title="NSF Sponsored Workshop in Software Engineering for Robotics">SE4Robotics'23</abbr></a>
			<ul class="nav">
				<li><a href="https://conf.researchr.org/home/icse-2023" target="_blank">Co-located with
				<abbr title="Internation Conference on Intelligent Robots and Systems">IROS</abbr> 2023</a>
				</li>
			</ul>
			<ul class="secondary-nav">
			<h5><a href="./cfp_pc.pdf">
			October 5-6, 2023, Detroit, MI
			</a></h5>
			</ul>
		</div>
	</div>
</div>

<div class="container">
	<div class="content">
		 
		<div class="row">
			<div class="span16">
				<p>&nbsp;</p><p>&nbsp;</p>
				<img src="./images/detroit.jpeg" style=' width: 100%;'/>
			</div>
		</div>
	
		<div class="hero-unit portada">
		   	<div style="float: right; width: 235px; margin: -25px;">
				<div class="calendar">
					<span class="month">May 2023</span>
					<span class="day">14</span>
				</div>
			</div>
			
			<h1><abbr title="International Workshop on Equitable Data and Technology">Fairware'23</abbr><br></h1>
			<p>
				The International Workshop on Equitable Data & Technology brings together academic researchers, industry researchers, and practitioners interested in exploring ways to build <b>fairer, more equitable</b>, data-driven software.
				
				<br/>
				<br/>
				
				Co-located with ICSE’23, the FairWare’23 meeting will include
				keynotes on software fairness form different perspectives. FairWare’23
				will also host panel sessions to invite researchers and
				the audience to engage in discussion.
				
				<br/>
				<br/>
				
				Since many issues associated with fairness are often sociological in
				nature, we welcome <b>commentaries from outside of computer
				science</b> that can shed light on the complex issue of fairness.
				
				<p style="padding: 0 0 0 0px;"><a href="./cfp_pc.pdf" class="btn primary large" target="_blank"><abbr title="Call For Papers">CFP</abbr> in PDF</a>
			</p>
		</div>
		
		<hr>
		
		<div class="row">
			<div class="span4">
				<h2>What is software fairness?</h2>
				As a society, we decide what attributes influence certain behavior. For example, race should not affect access to financial loans.
				Examples of real-world software exhibiting bias include image search and translation engines exhibiting gender stereotypes and facial detection and recognition tools’ depending on demographics.
			</div>
			<div class="span1">
				&nbsp;
			</div>
			<div class="span4">
				<h2>Is there research on software fairness?</h2>
				There are many software engineering challenges to building fair software that has not been addressed, from specifying fairness requirements to analysis, testing, and maintenance.
				FairWare 2023 will bring together academic and industry researchers and industry practitioners interested in creating software engineering technology to improve software fairness.
			</div>
			<div class="span1">
				&nbsp;
			</div>
			<div class="span6">
				<h2>Why do we need more research on software fairness?</h2>
				Recently, the requirements for fairer AI have become more common. The European Union, Microsoft, and the IEEE have all released white papers discussing fair and ethical AI.
				While these  documents differ in the details,  they all agree that  ethical AI must be ``FAT'';
				i.e., fair, accountable and transparent. Such fairer "FAT"er AI systems support five ``FAT'' items:
				
				<ul>
					<li> Integration with <b>human agency</b>
					<li> <b>Accountability</b> where conclusions are challenged
					<li> <b>Transparency</b> of how conclusions are made
					<li> Oversight on what must change to <b>fix bad conclusions</b>
					<li> <b>Inclusiveness</b> such that no specific segment of society is especially and unnecessarily privileged or discriminated against by the actions of the AI.
				</ul>
			</div>
		</div>
		
		<hr>
		
		<div class="row">
			<div class="span8">
				<h2>Special Issue <abbr title="Call For Papers">CFP</abbr></h2>
				Following on from the workshop, there will be a journal special issue at the Journal of Systems and Software: “Over the horizon: Limits and breakthroughs in algorithmic fairness. What are our next steps?” (Dates TBD). As far as possible, reviewers from Fairware'23 will be reused for the journal special issue (so authors should know what revisions are required to turn their Fairware'23 paper into a journal paper).
				<br/> <br/>
				
<!--				<p style="padding: 0 0 0 0px;"><a href="https://github.com/emsejournal/emsejournal.github.io/blob/master/special_issues/2022_Equitable_Data_and_Technology.md" class="btn primary large" target="_blank">Special Issue CFP</a>-->
			</div>
			<div class="span2">
				&nbsp;
			</div>
			<div class="span6">
				<h2>FairWare Resources</h2>
				The FairWare conference is over, and what an experience it was! We gathered many resources, open questions and ideas for future FairWare editions from our great discussions. These are available in the link below. You are welcome to leave new ideas or resources as well!
				
				<br/> <br/>
				
				<p style="padding: 0 0 0 0px;"><a href="https://docs.google.com/document/d/1Pt5PDyL8nPDJDRDyC2jJx4Lyb7xMUltKVVESgJk5Gg0/edit#" class="btn primary large" target="_blank">Link to resources</a>
			</div>
		</div>

		<hr>

		<div class="row">
			<div class="span16">
			<img align=right src=".\images\Keyes.png" width=400>
				<h2>Keynote: Fairness through Unfairness </h2>
					<p> Producing fair outcomes in a structural sense may actually require deliberately weighting software in favour of marginalised populations. 
						The question of how to make algorithmic systems fair is a common one for researchers concerned with the social consequences of 
						automation and machine learning. But what if it is the wrong question? What if the right one is to ask how we might make things unfair?
						In this talk, I will posit precisely that. Drawing on illustrative examples from housing to hiring, along with the history of 
						fairness as a concept, I will argue that - taking into account the broader contexts of algorithmic systems - achieving fair 
						outcomes may require developers to work towards unfair outcomes, first. </p>
				<br/>
				<p>Speaker:  <b>Os Keyes </b> <a href=" https://ironholds.org/"> https://ironholds.org/</a><br>
					Os Keyes is a PhD Candidate at the University of Washington’s Department of Human Centred Design & Engineering. <b>Details TBD!</b>
			</div>
			 
		</div>

        <hr>
		<div class="row">
			<div class="span16">
			<img align=right src=".\images\austin_photo.jpg" width=400>
				<h2>Keynote: Seldonian Toolkit </h2>
					<p> Software systems that use machine learning are routinely deployed in a wide range of settings, including medical applications, the criminal
					justice system, hiring, facial recognition, social media, and advertising.
					These systems can produce unsafe and unfair behavior, such as suggesting
					harmful medical treatments, making racist or sexist recommendations, and
					facilitating radicalization and polarization in society.
					To address this, we developed the Seldonian
					Toolkit for training machine learning models that adhere to fairness and safety requirements. The models the toolkit
					produces are probabilistically verified: they are guaranteed, with high
					probability, to satisfy the specified safety or fairness requirements even
					when applied to previously unseen data. The toolkit is a set of open source Python packages which are available to download. A video demonstrating the
					Seldonian Toolkit is available at <a href="https://youtu.be/wHR-hDm9jX4/"> https://youtu.be/wHR-hDm9jX4/</a> . </p>
				<br/>
				<p>Speaker:  <b>Austin Hoag </b> <br>
					Dr. Austin Hoag is a machine learning engineer at the Berkeley Existential Risk Initiative (BERI),
					a nonprofit that collaborates with university research groups working to reduce existential risk.
					He is the co-creator and lead software engineer for the Seldonian Toolkit, a collaboration with
					machine learning researchers at the University of Massachusetts. Before BERI, he worked as a software
					developer at the Princeton Neuroscience Institute at Princeton University. He received his PhD in
					Physics from the University of California, Davis in 2018 and conducted postdoctoral research in astrophysics at UCLA.
			</div>

		</div>

        <hr>


		<div class="row">
			<div class="span9">
				<h2>Paper Submission</h2>
				<p>
				Papers will be submitted through <a href="https://fairware23.hotcrp.com/">HotCRP</a>, and will be subjected to <b>double-blind</b> reviews.
				Submissions must use the official “ACM Primary Article Template” from the <a href="http://www.acm.org/publications/proceedings-template">ACM proceedings template</a>.
				LaTeX users should use the sigconf option, as well as the review (to produce line numbers for easy reference by the reviewers) and anonymous (omitting author names) options.
				In addition, submitted papers must not exceed the 8-page limit, be written in English, must present an original contribution, and must not be published or under review elsewhere.
				</p>

				<p>
				Two members of the program committee will review
				each paper and the committee will select the papers for
				presentation at workshop based on quality, relevance, and
				the potential for starting meaningful and productive conversations.
				</p>

				<h2>Workshop Participation</h2>

				<p>
				At least one author of each accepted paper must register for the
				workshop. Each paper will be presented in a 15-20 minute presentation with follow-up questions and discussion.
				</p>

<!--				<h2>Special Issue</h2>-->
<!--				<p>-->
<!--					<b> TBD </b>-->
<!--				Following on from the workshop, there will be an open call for a special journal issue on fair and equitable data and technology.-->
<!--				In that special issue, reviewers from this workshop will review extended versions of the FairWare'22 papers. For more-->
<!--					details see our <a -->
<!--		              href="https://emsejournal.github.io/special_issues/2022_Equitable_Data_and_Technology.html">journal special issue call for papers</a>.-->
				</p>

			</div>

			<div class="span1">
				<p>&nbsp;</p>
			</div>

			<div class="span6">


				<h2>Important Dates</h2> <ul>
					<li>Submission:
					<strong> 25 Jan </strong></li>
					<li>Notification of acceptance:
					<strong>24 Feb</strong></li>
					<li>Camera-ready submission:
					<strong>17 Mar</strong></li>
					<li>Workshop date: <strong>2X
					May</strong></li>
<!--					<li>Submission for follow-up journal special issue: -->
<!--					<strong>Sept 15, 2022 [<a -->
<!--		              href="https://emsejournal.github.io/special_issues/2022_Equitable_Data_and_Technology.html">details</a>]</strong>-->
				</ul>
			</div>
		</div>


		<hr>

        <div class="row">
            <div class="span16">
                <h2>Schedule</h2>
				<br> 8:45 - 9:00 <strong>Welcome from the organizers</strong> </br>
				<br> 9:00 - 10:00 <strong>Keynote</strong> </br>
					<br> &nbsp; &nbsp; &nbsp;  “Fairness through Unfairness” by Os Keyes, University of Washington </br>
				<br> 10:00 - 10:30 <strong>Paper Session 1</strong> </br>
					<br> &nbsp; &nbsp; &nbsp; “Fair-Siamese Approach for Accurate Fairness in Image Classification” by Kwanhyong Lee, Van-Thuan Pham, and Jiayuan He </br>
        			<br> 10:30 - 11:00 <strong>Morning COFFEE</strong> </br>
				<br> 11:00 - 12:00 <strong>Paper Session 2</strong> </br>
					<br> &nbsp; &nbsp; &nbsp; “On Retrofitting Provenance for Transparent and Fair Software – Drivers and Challenges” by Jens Dietrich, Matthias Galster, and Markus Luczak-Roesch </br>
					<br> &nbsp; &nbsp; &nbsp; “Heavy-tailed Uncertainty in AI Policy” by Lelia Marie Hampton  </br>
				<br> 12:00 - 12:30 <strong>Tutorial</strong> </br>
					<br> &nbsp; &nbsp; &nbsp; Quantitative and Qualitative Methods for Equitable Research and Development </br>
				<br> 12:30 - 13:45 <strong>LUNCH break (and networking!)</strong> </br>
				<br> 13:45 - 14:45 <strong>Keynote</strong> </br>
					<br> &nbsp; &nbsp; &nbsp; “Applying Safe and Fair Machine Learning Algorithms with the Seldonian Toolkit” by Austin Hoag, Berkeley Existential Risk Initiative (BERI) </br>
				<br> 14:45 - 15:15 <strong>Paper Session 3</strong> </br>
					<br> &nbsp; &nbsp; &nbsp;  “Reflexive Practices in Software Engineering” by Alicia Boyd </br>
				<br> 15:15 - 15:45 <strong>Afternoon COFFEE</strong> </br>
				<br> 15:45 - 17:05 <strong>Workshop in a workshop: Teaching Ethics</strong> </br>
				<br> 17:05 - 17:15 <strong>Workshop Closing + Dinner plans</strong> </br>
        </div>
            </div>
        <hr>



		<div class="row">
			<div class="span14">
				<h2>Topics of Interest</h2>
<!--				<ul>-->
<!--					<li><b>Socio-technical challenges</b></li>-->
<!--					<ul>-->
<!--						<li>How to determine the <b>trade-off</b> between making fair(er) systems and other objectives of a system?</li>-->
<!--						<li>How do we build equitable software given the inherently unfair social pressures behind it?</li>-->
<!--					</ul>-->
<!--					<li><b>Algorithmic challenges</b></li>-->
<!--					<ul>-->
<!--						<li>How to <b>identify bias</b> in AI models?</li>-->
<!--						<li>How to <b>explain</b> the source or reason for this bias?</li>-->
<!--						<li>How to <b>measure</b> the level of bias in systems?</li>-->
<!--						<li>How to <b>mitigate</b> the effect of this bias by changing how models are trained?</li>-->
<!--						<li>How to <b>provide</b> support for explanation of automated decisions and redress for stakeholders and other mechanisms for accountability and transparency of deployed systems?</li>-->
<!--					</ul>-->
<!--				</ul>-->
				<p>
					To support fairer “FAT”er software we aims to empower software developers, individuals and organizations,
					with methods and tools that <b>measure</b>, <b>manage</b>, and <b>mitigate</b> unfairness.
					Therefore we ask for papers that explore:
					<ul>
				<li>How to <b>identify</b> bias in AI models?</li>
				<li>How to <b>explain</b> the source or reason for this bias?</li>
				<li>How to <b>measure</b> the level of bias on these systems?</li>
				<li>How to <b>mitigate</b> bias by changing model training?</li>
				<li> How to <b>support</b> for explanations of automated decisions
						and redress for stakeholders for accountability and transparency of deployed systems? </li>
				<li> How to <b>determine</b> the trade-off between making fair(er) systems and other objectives of a system?</li>
				<li> Are there <b>inherently unfair social pressures</b> that doom us to forever delivering unfair software?</li>
					</ul>
				</p>
				<p>
					We are accepting contributions as full papers (4--8 pages),
					with either novel research results or a statement of vision or position, on one or more of the following perspectives:
					<ul>
						<li> <b>Improving fairness</b> -- Present a novel approach or evaluate
						an existing approach for software fairness. This can be along
						the lines, but not limited to identification, explanation, measurement, and mitigation of fairness.</li>
						<li><b>Applying fairness</b> -- artificial intelligence, machine learning,
						requirements and design, testing, software engineering cycle,
						and policy-making, among many other areas of interest.</li>
						<li><b>Pose challenges</b> -- Show the weak points in fairness methods, and lead the way on the path to novel research.
						Request new models, processes, metrics, and artifacts.</li>
						<li><b>Collaboration studies</b> -- Between researchers & industry,
						across the industry, across domains and disciplines, or col-
						laborations between research groups.</li>
					</ul>
				</p>

			</div>
		</div>

		<hr>


		
	
		<div class="row">
			<div class="span10">

				<h2>Programme Committee</h2>

				<li>Joymallya Chakraborty, Amazon</li>
				<li>Alex Groce, Northern Arizona University</li>
				<li>Christine Julien, University of Texas at Austin</li>
				<li>Os Keyes, University of Washington</li>
				<li>Rahul Pandita, GitHub</li>
				<li>Siobahn Day Grady, North Carolina Central University</li>
				<li>Gema Rodriguez-Perez, University British Columbia</li>
				<li>Muhammad Ali Gulzar, Virginia Tech</li>
				<li>Mei Nagappan, University of Waterloo</li>
				<li>Kevin Moran, George Mason University</li>
				<li>Lelia Marie Hampton, Massachusetts Institute of Technology</li>
				<li>Robert DeLine, Microsoft Research</li>
				<li>Marc Canellas, Office of the Public Defender for Arlington County and the City of Falls Church</li>
				<li>Mats Heimdahl, University of Minnesota</li>


						
					</ul>
				</div>
			<div class="span6">
				<h2>Organizing Committee</h2>
				<ul>
					<li>Brittany Johnson, George Mason University, USA</li>
					<li>Tim Menzies, NC State University, USA</li>
					<li>Federica Sarro, University College, UK.</li>
					<li>Zhe Yu, Rochester Institute of Technology, USA</li>
					<li>Yuriy Brun, U.Massachusetts, USA</li>
					<li>Jeanna Matthews, Clarkson University, USA</li>
					<li>Alicia Boyd, DePaul University, USA</li>
					<li>Justin Smith, Lafayette College, USA</li>

				</ul>
			</div>
		</div>
	</div>
	
	<hr>
	
	<div class="row">
		<div class="span10">
			<h2>Previous editions</h2>
			<ul>
			<li>2018 - <a href="https://fairware.cs.umass.edu/" target="_blank">https://fairware.cs.umass.edu/</a> - <a href="https://dl.acm.org/doi/proceedings/10.1145/3194770" target="_blank">ACM DL</a>
			<li>2022 - <a href="https://github.com/fairwares/fairwares.github.io/blob/main/docs/images/Fairware'22.pdf" target="_blank">https://github.com/fairwares/fairwares.github.io/blob/main/docs/images/Fairware'22.pdf/</a>
			</ul>
		</div>
	</div>
	
	<footer>
		<div class="secondary-nav">
			<p><abbr title="International Workshop on Equitable Data and Technology">FairWare</abbr>'23</a>
		</div>
	</p>
	</footer>
</div>

</body>
</html>
